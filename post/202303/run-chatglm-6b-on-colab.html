<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><title>Run ChatGLM-6B on Colab</title><meta name="next-head-count" content="3"/><meta name="description" content="Generated by create next app"/><link rel="icon" href="/favicon.ico"/><link rel="preload" href="/_next/static/css/dc76b23cea82c435.css" as="style"/><link rel="stylesheet" href="/_next/static/css/dc76b23cea82c435.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-f11614d8aa7ee555.js" defer=""></script><script src="/_next/static/chunks/pages/_app-95d34612df878707.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5B...routes%5D-3240d32ee1486cbf.js" defer=""></script><script src="/_next/static/HkcMTPpvc8PJ5ksTEEHTj/_buildManifest.js" defer=""></script><script src="/_next/static/HkcMTPpvc8PJ5ksTEEHTj/_ssgManifest.js" defer=""></script></head><body><div class="flex min-h-screen w-screen items-center justify-center"><div id="__next"><main class="container m-4 flex max-w-[100vw] flex-col items-stretch gap-4 rounded-lg bg-white p-4 shadow-lg sm:m-8 sm:p-8"><div class="markdown"><h1 id="run-chatglm-6b-on-colab"><a aria-hidden="true" tabindex="-1" href="#run-chatglm-6b-on-colab"><span class="icon icon-link"></span></a>Run ChatGLM-6B on Colab</h1>
<p>Recently <a href="https://github.com/THUDM/ChatGLM-6B">ChatGLM-6B</a> is a bit on trend. I wanna give it a try but unlucky my graphics card is just too old to fullfill its needs.</p>
<p>As always, I want to test it on Colab before going further.</p>
<h2 id="what-is-colab"><a aria-hidden="true" tabindex="-1" href="#what-is-colab"><span class="icon icon-link"></span></a>What is Colab?</h2>
<p>ChatGPT says:</p>
<blockquote>
<p>Colab, Google Colab, short for "Google Colaboratory," is a free online platform provided by Google that allows users to write and run Python code in a web browser without the need for any installation or configuration. It provides a virtual computing environment with access to CPUs, GPUs, and TPUs (Tensor Processing Units), making it possible to run machine learning algorithms and other computationally intensive tasks.</p>
</blockquote>
<p>What's more, free users can try it a few hours everyday, and the environment are easy to reset, making it a very good platform to learn and test tasks that require computing powers.</p>
<h2 id="try-it-now"><a aria-hidden="true" tabindex="-1" href="#try-it-now"><span class="icon icon-link"></span></a>Try it now</h2>
<p><a href="https://colab.research.google.com/drive/1S8w0pbOsUHU1JqJzoWQKNgvG6HBwbI6c">https://colab.research.google.com/drive/1S8w0pbOsUHU1JqJzoWQKNgvG6HBwbI6c</a></p>
<h2 id="prerequisites"><a aria-hidden="true" tabindex="-1" href="#prerequisites"><span class="icon icon-link"></span></a>Prerequisites</h2>
<h3 id="prepare-test-code-and-actual-model"><a aria-hidden="true" tabindex="-1" href="#prepare-test-code-and-actual-model"><span class="icon icon-link"></span></a>Prepare test code and actual model</h3>
<p>Clone test code:</p>
<pre><code class="language-jupyter">%cd /content
!git clone https://github.com/THUDM/ChatGLM-6B
</code></pre>
<p>Clone actual model:</p>
<pre><code>%cd /content/ChatGLM-6B
!git clone --depth=1 https://huggingface.co/THUDM/chatglm-6b THUDM/chatglm-6b
</code></pre>
<p>Install dependencies:</p>
<pre><code>!pip install -r requirements.txt
!pip install gradio
!pip install accelerate
</code></pre>
<h2 id="troubleshooting"><a aria-hidden="true" tabindex="-1" href="#troubleshooting"><span class="icon icon-link"></span></a>Troubleshooting</h2>
<p>Normally, after prerequisites, the following prompts should get it running.</p>
<pre><code>%cd /content/ChatGLM-6B
!python3 web_demo.py
</code></pre>
<p>Colab offers a small memory capacity, which is only 13G, and the loading approach of the test code will simply crash because of Out Of Memory.</p>
<p>So I search a bit and discover that we can optimize the loading process to reduce memory usage, in:</p>
<ul>
<li>https://huggingface.co/docs/accelerate/usage_guides/big_modeling</li>
<li>https://github.com/huggingface/blog/blob/main/accelerate-large-models.md</li>
</ul>
<p>Firstly, I modify the loading process like the first link, which uses <code>device_map='auto'</code>, and while the first try succeeds, it takes up all GPU memory and fails to replicate.</p>
<p>Then I discover that <code>device_map</code> is actually the config to instruct which layer should run on which device. After printing out the result of <code>infer_auto_device_map()</code>, it's interesting to know that the generate <code>device_map</code> is empty, which makes everything goes to GPU and boom.</p>
<p>After some try and error, I end up with a working <code>device_map</code>, with correct <code>no_split_module_classes</code>, and it finally works.</p>
<h2 id="performance"><a aria-hidden="true" tabindex="-1" href="#performance"><span class="icon icon-link"></span></a>Performance</h2>
<p>On my end, the Colab link above gives around 5 minutes to setup presequisites, and around 5 minutes before Gradio shows up.</p>
<p>The memory usage is about 7.7G, and the GPU memory usage is around 12.7G, which becomes 13.8G after 20 messages. The response time is 30-40s, sometimes 60s, which is acceptable for testing purpose.</p>
<h2 id="further-optimization"><a aria-hidden="true" tabindex="-1" href="#further-optimization"><span class="icon icon-link"></span></a>Further optimization</h2>
<p>I fail to use <code>quantize(4)</code>, which is said to reduce memory usage, and I don't know if missing <code>cuda()</code> has impact on performance.</p>
<p>The <code>device_map</code> is a cool thing to tweak with, if you have multiple CUDA devices, it should enable you to distribute workloads across them.</p></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"yyyymm":"202303","name":"run-chatglm-6b-on-colab","contentHtml":"\u003ch1 id=\"run-chatglm-6b-on-colab\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#run-chatglm-6b-on-colab\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun ChatGLM-6B on Colab\u003c/h1\u003e\n\u003cp\u003eRecently \u003ca href=\"https://github.com/THUDM/ChatGLM-6B\"\u003eChatGLM-6B\u003c/a\u003e is a bit on trend. I wanna give it a try but unlucky my graphics card is just too old to fullfill its needs.\u003c/p\u003e\n\u003cp\u003eAs always, I want to test it on Colab before going further.\u003c/p\u003e\n\u003ch2 id=\"what-is-colab\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#what-is-colab\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat is Colab?\u003c/h2\u003e\n\u003cp\u003eChatGPT says:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eColab, Google Colab, short for \"Google Colaboratory,\" is a free online platform provided by Google that allows users to write and run Python code in a web browser without the need for any installation or configuration. It provides a virtual computing environment with access to CPUs, GPUs, and TPUs (Tensor Processing Units), making it possible to run machine learning algorithms and other computationally intensive tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhat's more, free users can try it a few hours everyday, and the environment are easy to reset, making it a very good platform to learn and test tasks that require computing powers.\u003c/p\u003e\n\u003ch2 id=\"try-it-now\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#try-it-now\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTry it now\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://colab.research.google.com/drive/1S8w0pbOsUHU1JqJzoWQKNgvG6HBwbI6c\"\u003ehttps://colab.research.google.com/drive/1S8w0pbOsUHU1JqJzoWQKNgvG6HBwbI6c\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"prerequisites\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#prerequisites\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003ch3 id=\"prepare-test-code-and-actual-model\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#prepare-test-code-and-actual-model\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrepare test code and actual model\u003c/h3\u003e\n\u003cp\u003eClone test code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-jupyter\"\u003e%cd /content\n!git clone https://github.com/THUDM/ChatGLM-6B\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eClone actual model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e%cd /content/ChatGLM-6B\n!git clone --depth=1 https://huggingface.co/THUDM/chatglm-6b THUDM/chatglm-6b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eInstall dependencies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e!pip install -r requirements.txt\n!pip install gradio\n!pip install accelerate\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"troubleshooting\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#troubleshooting\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting\u003c/h2\u003e\n\u003cp\u003eNormally, after prerequisites, the following prompts should get it running.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e%cd /content/ChatGLM-6B\n!python3 web_demo.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eColab offers a small memory capacity, which is only 13G, and the loading approach of the test code will simply crash because of Out Of Memory.\u003c/p\u003e\n\u003cp\u003eSo I search a bit and discover that we can optimize the loading process to reduce memory usage, in:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ehttps://huggingface.co/docs/accelerate/usage_guides/big_modeling\u003c/li\u003e\n\u003cli\u003ehttps://github.com/huggingface/blog/blob/main/accelerate-large-models.md\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFirstly, I modify the loading process like the first link, which uses \u003ccode\u003edevice_map='auto'\u003c/code\u003e, and while the first try succeeds, it takes up all GPU memory and fails to replicate.\u003c/p\u003e\n\u003cp\u003eThen I discover that \u003ccode\u003edevice_map\u003c/code\u003e is actually the config to instruct which layer should run on which device. After printing out the result of \u003ccode\u003einfer_auto_device_map()\u003c/code\u003e, it's interesting to know that the generate \u003ccode\u003edevice_map\u003c/code\u003e is empty, which makes everything goes to GPU and boom.\u003c/p\u003e\n\u003cp\u003eAfter some try and error, I end up with a working \u003ccode\u003edevice_map\u003c/code\u003e, with correct \u003ccode\u003eno_split_module_classes\u003c/code\u003e, and it finally works.\u003c/p\u003e\n\u003ch2 id=\"performance\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#performance\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePerformance\u003c/h2\u003e\n\u003cp\u003eOn my end, the Colab link above gives around 5 minutes to setup presequisites, and around 5 minutes before Gradio shows up.\u003c/p\u003e\n\u003cp\u003eThe memory usage is about 7.7G, and the GPU memory usage is around 12.7G, which becomes 13.8G after 20 messages. The response time is 30-40s, sometimes 60s, which is acceptable for testing purpose.\u003c/p\u003e\n\u003ch2 id=\"further-optimization\"\u003e\u003ca aria-hidden=\"true\" tabindex=\"-1\" href=\"#further-optimization\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFurther optimization\u003c/h2\u003e\n\u003cp\u003eI fail to use \u003ccode\u003equantize(4)\u003c/code\u003e, which is said to reduce memory usage, and I don't know if missing \u003ccode\u003ecuda()\u003c/code\u003e has impact on performance.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003edevice_map\u003c/code\u003e is a cool thing to tweak with, if you have multiple CUDA devices, it should enable you to distribute workloads across them.\u003c/p\u003e","title":"Run ChatGLM-6B on Colab"}},"__N_SSG":true},"page":"/post/[...routes]","query":{"routes":["202303","run-chatglm-6b-on-colab"]},"buildId":"HkcMTPpvc8PJ5ksTEEHTj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>