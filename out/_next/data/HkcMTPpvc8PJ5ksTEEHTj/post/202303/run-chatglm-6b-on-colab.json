{"pageProps":{"postData":{"yyyymm":"202303","name":"run-chatglm-6b-on-colab","contentHtml":"<h1 id=\"run-chatglm-6b-on-colab\"><a aria-hidden=\"true\" tabindex=\"-1\" href=\"#run-chatglm-6b-on-colab\"><span class=\"icon icon-link\"></span></a>Run ChatGLM-6B on Colab</h1>\n<p>Recently <a href=\"https://github.com/THUDM/ChatGLM-6B\">ChatGLM-6B</a> is a bit on trend. I wanna give it a try but unlucky my graphics card is just too old to fullfill its needs.</p>\n<p>As always, I want to test it on Colab before going further.</p>\n<h2 id=\"what-is-colab\"><a aria-hidden=\"true\" tabindex=\"-1\" href=\"#what-is-colab\"><span class=\"icon icon-link\"></span></a>What is Colab?</h2>\n<p>ChatGPT says:</p>\n<blockquote>\n<p>Colab, Google Colab, short for \"Google Colaboratory,\" is a free online platform provided by Google that allows users to write and run Python code in a web browser without the need for any installation or configuration. It provides a virtual computing environment with access to CPUs, GPUs, and TPUs (Tensor Processing Units), making it possible to run machine learning algorithms and other computationally intensive tasks.</p>\n</blockquote>\n<p>What's more, free users can try it a few hours everyday, and the environment are easy to reset, making it a very good platform to learn and test tasks that require computing powers.</p>\n<h2 id=\"try-it-now\"><a aria-hidden=\"true\" tabindex=\"-1\" href=\"#try-it-now\"><span class=\"icon icon-link\"></span></a>Try it now</h2>\n<p><a href=\"https://colab.research.google.com/drive/1S8w0pbOsUHU1JqJzoWQKNgvG6HBwbI6c\">https://colab.research.google.com/drive/1S8w0pbOsUHU1JqJzoWQKNgvG6HBwbI6c</a></p>\n<h2 id=\"prerequisites\"><a aria-hidden=\"true\" tabindex=\"-1\" href=\"#prerequisites\"><span class=\"icon icon-link\"></span></a>Prerequisites</h2>\n<h3 id=\"prepare-test-code-and-actual-model\"><a aria-hidden=\"true\" tabindex=\"-1\" href=\"#prepare-test-code-and-actual-model\"><span class=\"icon icon-link\"></span></a>Prepare test code and actual model</h3>\n<p>Clone test code:</p>\n<pre><code class=\"language-jupyter\">%cd /content\n!git clone https://github.com/THUDM/ChatGLM-6B\n</code></pre>\n<p>Clone actual model:</p>\n<pre><code>%cd /content/ChatGLM-6B\n!git clone --depth=1 https://huggingface.co/THUDM/chatglm-6b THUDM/chatglm-6b\n</code></pre>\n<p>Install dependencies:</p>\n<pre><code>!pip install -r requirements.txt\n!pip install gradio\n!pip install accelerate\n</code></pre>\n<h2 id=\"troubleshooting\"><a aria-hidden=\"true\" tabindex=\"-1\" href=\"#troubleshooting\"><span class=\"icon icon-link\"></span></a>Troubleshooting</h2>\n<p>Normally, after prerequisites, the following prompts should get it running.</p>\n<pre><code>%cd /content/ChatGLM-6B\n!python3 web_demo.py\n</code></pre>\n<p>Colab offers a small memory capacity, which is only 13G, and the loading approach of the test code will simply crash because of Out Of Memory.</p>\n<p>So I search a bit and discover that we can optimize the loading process to reduce memory usage, in:</p>\n<ul>\n<li>https://huggingface.co/docs/accelerate/usage_guides/big_modeling</li>\n<li>https://github.com/huggingface/blog/blob/main/accelerate-large-models.md</li>\n</ul>\n<p>Firstly, I modify the loading process like the first link, which uses <code>device_map='auto'</code>, and while the first try succeeds, it takes up all GPU memory and fails to replicate.</p>\n<p>Then I discover that <code>device_map</code> is actually the config to instruct which layer should run on which device. After printing out the result of <code>infer_auto_device_map()</code>, it's interesting to know that the generate <code>device_map</code> is empty, which makes everything goes to GPU and boom.</p>\n<p>After some try and error, I end up with a working <code>device_map</code>, with correct <code>no_split_module_classes</code>, and it finally works.</p>\n<h2 id=\"performance\"><a aria-hidden=\"true\" tabindex=\"-1\" href=\"#performance\"><span class=\"icon icon-link\"></span></a>Performance</h2>\n<p>On my end, the Colab link above gives around 5 minutes to setup presequisites, and around 5 minutes before Gradio shows up.</p>\n<p>The memory usage is about 7.7G, and the GPU memory usage is around 12.7G, which becomes 13.8G after 20 messages. The response time is 30-40s, sometimes 60s, which is acceptable for testing purpose.</p>\n<h2 id=\"further-optimization\"><a aria-hidden=\"true\" tabindex=\"-1\" href=\"#further-optimization\"><span class=\"icon icon-link\"></span></a>Further optimization</h2>\n<p>I fail to use <code>quantize(4)</code>, which is said to reduce memory usage, and I don't know if missing <code>cuda()</code> has impact on performance.</p>\n<p>The <code>device_map</code> is a cool thing to tweak with, if you have multiple CUDA devices, it should enable you to distribute workloads across them.</p>","title":"Run ChatGLM-6B on Colab"}},"__N_SSG":true}